{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangcheng/anaconda3/envs/swin/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import VisionEncoderDecoderModel,GPT2TokenizerFast,ViTImageProcessor,Seq2SeqTrainer,Seq2SeqTrainingArguments\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "from peft import get_peft_model, LoraConfig,TaskType\n",
    "from textwrap import wrap\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = 'microsoft/swin-base-patch4-window7-224-in22k'\n",
    "decoder_model ='gpt2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/swin-base-patch4-window7-224-in22k were not used when initializing SwinModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_proj.bias', 'h.6.crossattention.c_attn.bias', 'h.9.ln_cross_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.7.crossattention.q_attn.bias', 'h.7.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.5.ln_cross_attn.weight', 'h.2.crossattention.c_proj.weight', 'h.1.crossattention.c_proj.weight', 'h.4.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.8.ln_cross_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.1.ln_cross_attn.weight', 'h.6.ln_cross_attn.weight', 'h.0.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.0.ln_cross_attn.bias', 'h.8.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.weight', 'h.3.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.4.crossattention.c_attn.weight', 'h.7.ln_cross_attn.weight', 'h.10.crossattention.c_attn.weight', 'h.2.crossattention.c_attn.weight', 'h.1.crossattention.q_attn.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.3.crossattention.c_attn.bias', 'h.3.ln_cross_attn.bias', 'h.4.ln_cross_attn.bias', 'h.8.ln_cross_attn.bias', 'h.7.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.weight', 'h.9.crossattention.c_attn.weight', 'h.8.crossattention.q_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_proj.weight', 'h.1.crossattention.c_proj.bias', 'h.7.crossattention.c_attn.weight', 'h.0.ln_cross_attn.weight', 'h.4.crossattention.q_attn.bias', 'h.7.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.q_attn.bias', 'h.10.ln_cross_attn.weight', 'h.2.crossattention.q_attn.weight', 'h.5.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.bias', 'h.0.crossattention.c_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.ln_cross_attn.bias', 'h.7.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.bias', 'h.9.ln_cross_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.0.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.weight', 'h.0.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.8.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.2.ln_cross_attn.weight', 'h.1.crossattention.q_attn.weight', 'h.11.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.8.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.2.ln_cross_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.7.crossattention.q_attn.weight', 'h.10.crossattention.c_attn.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    encoder_model,\n",
    "    decoder_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained(decoder_model)\n",
    "image_processor =ViTImageProcessor.from_pretrained(encoder_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"gpt2\" in decoder_model:\n",
    "  # gpt2 does not have decoder_start_token_id and pad_token_id\n",
    "  # but has bos_token_id and eos_token_id\n",
    "  tokenizer.pad_token = tokenizer.eos_token # pad_token_id as eos_token_id\n",
    "  model.config.eos_token_id = tokenizer.eos_token_id\n",
    "  model.config.pad_token_id = tokenizer.pad_token_id\n",
    "  # set decoder_start_token_id as bos_token_id\n",
    "  model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "else:\n",
    "  # set the decoder start token id to the CLS token id of the tokenizer\n",
    "  model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "  # set the pad token id to the pad token id of the tokenizer\n",
    "  model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peft_config = LoraConfig(task_type='image_caption',\n",
    "#                          target_modules = [\"query\", \"value\"],\n",
    "#                          inference_mode=False, \n",
    "#                          r=8, \n",
    "#                          lora_alpha=32, \n",
    "#                          lora_dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_peft_model(model,peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5897/5897 [00:00<00:00, 6937.12it/s] \n",
      "Found cached dataset imagefolder (/home/wangcheng/.cache/huggingface/datasets/imagefolder/total-b9b0b8db427296d7/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.28it/s]\n"
     ]
    }
   ],
   "source": [
    "root='/mnt/storage-ssd/wangcheng/dataset/rgb/GIT/total/'\n",
    "\n",
    "ds = load_dataset(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds['train'].train_test_split(0.1)\n",
    "test_ds = ds['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds['train'].train_test_split(0.15)\n",
    "train_ds = ds['train']\n",
    "valid_ds = ds['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4510, 796, 590)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds),len(valid_ds),len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 16 # max length of the captions in tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(items):\n",
    "  # preprocess the image\n",
    "  pixel_values = image_processor(items[\"image\"], return_tensors=\"pt\").pixel_values.to(device)\n",
    "  # tokenize the caption with truncation and padding\n",
    "  targets = tokenizer([ sentence[\"raw\"] for sentence in items[\"sentences\"] ], \n",
    "                      max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\").to(device)\n",
    "  return {'pixel_values': pixel_values, 'labels': targets[\"input_ids\"]}\n",
    "\n",
    "# using with_transform to preprocess the dataset during training\n",
    "train_dataset = train_ds.with_transform(preprocess)\n",
    "valid_dataset = valid_ds.with_transform(preprocess)\n",
    "test_dataset  = test_ds.with_transform(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(items):\n",
    "  # preprocess the image\n",
    "  pixel_values = image_processor(items[\"image\"], return_tensors=\"pt\").pixel_values.to(device)\n",
    "  # tokenize the caption with truncation and padding\n",
    "  targets = tokenizer(items['text'], \n",
    "                      max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\").to(device)\n",
    "#   inputs = image_processor(images=pixel_values, text=targets, padding=\"max_length\")\n",
    "#   inputs.update({\"labels\": inputs[\"input_ids\"]})\n",
    "#   return inputs\n",
    "  return {'pixel_values': pixel_values, 'labels': targets[\"input_ids\"]}\n",
    "\n",
    "train_dataset = train_ds.with_transform(preprocess)\n",
    "valid_dataset = valid_ds.with_transform(preprocess)\n",
    "test_dataset  = test_ds.with_transform(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function we'll use to collate the batches\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.stack([x['labels'] for x in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "# load the rouge and bleu metrics\n",
    "rouge = evaluate.load(\"rouge\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bleu_script.bleu import Bleu\n",
    "bleu =Bleu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "  preds = eval_pred.label_ids\n",
    "  labels = eval_pred.predictions\n",
    "  # decode the predictions and labels\n",
    "  pred_str = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "  labels_str = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "  # compute the rouge score\n",
    "  rouge_result = rouge.compute(predictions=pred_str, references=labels_str)\n",
    "  # multiply by 100 to get the same scale as the rouge score\n",
    "  rouge_result = {k: round(v * 100, 4) for k, v in rouge_result.items()}\n",
    "  # compute the bleu score\n",
    "  bleu_result = bleu.compute(predictions=pred_str, references=labels_str)\n",
    "  # get the length of the generated captions\n",
    "  generation_length = bleu_result[\"translation_length\"]\n",
    "  return {\n",
    "        **rouge_result, \n",
    "        \"bleu\": round(bleu_result[\"bleu\"] * 100, 4), \n",
    "        \"gen_len\": bleu_result[\"translation_length\"] / len(preds)\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2 # number of epochs\n",
    "batch_size = 16 # the size of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,             # use generate to calculate the loss\n",
    "    num_train_epochs=num_epochs,            # number of epochs\n",
    "    evaluation_strategy=\"steps\",            # evaluate after each eval_steps\n",
    "    eval_steps=20,                        # evaluate after each 500 steps\n",
    "    logging_steps=20,                     # log after each 500 steps\n",
    "    save_steps=20,                       # save after each 500 steps\n",
    "    per_device_train_batch_size=batch_size, # batch size for training\n",
    "    per_device_eval_batch_size=batch_size,  # batch size for evaluation\n",
    "    output_dir=\"vit-swin-base-224-gpt2-galaxy-captioning\", # output directory\n",
    "    # push_to_hub=True # whether you want to push the model to the hub,\n",
    "    # check this guide for more details: https://huggingface.co/transformers/model_sharing.html\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,                     # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    tokenizer=image_processor,       # we use the image processor as the tokenizer\n",
    "    args=training_args,              # pass the training arguments\n",
    "    compute_metrics=compute_metrics, \n",
    "    train_dataset=train_dataset,     \n",
    "    eval_dataset=valid_dataset,      \n",
    "    data_collator=collate_fn,        \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_eval_loader(eval_dataset=None):\n",
    "  return DataLoader(valid_dataset, collate_fn=collate_fn, batch_size=batch_size)\n",
    "\n",
    "def get_test_loader(eval_dataset=None):\n",
    "  return DataLoader(test_dataset, collate_fn=collate_fn, batch_size=batch_size)\n",
    "\n",
    "# override the get_train_dataloader, get_eval_dataloader and\n",
    "# get_test_dataloader methods of the trainer\n",
    "# so that we can properly load the data\n",
    "trainer.get_train_dataloader = lambda: DataLoader(train_dataset, collate_fn=collate_fn, batch_size=batch_size)\n",
    "trainer.get_eval_dataloader = get_eval_loader\n",
    "trainer.get_test_dataloader = get_test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangcheng/anaconda3/envs/swin/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/wangcheng/anaconda3/envs/swin/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='564' max='564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [564/564 49:10, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.067000</td>\n",
       "      <td>0.296111</td>\n",
       "      <td>64.560400</td>\n",
       "      <td>37.047100</td>\n",
       "      <td>64.544600</td>\n",
       "      <td>64.610000</td>\n",
       "      <td>37.865400</td>\n",
       "      <td>4.346734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.269400</td>\n",
       "      <td>0.139330</td>\n",
       "      <td>64.290100</td>\n",
       "      <td>36.423800</td>\n",
       "      <td>64.263500</td>\n",
       "      <td>64.315900</td>\n",
       "      <td>36.882100</td>\n",
       "      <td>4.346734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.198100</td>\n",
       "      <td>0.127372</td>\n",
       "      <td>70.743900</td>\n",
       "      <td>47.361400</td>\n",
       "      <td>70.709900</td>\n",
       "      <td>70.760300</td>\n",
       "      <td>44.414600</td>\n",
       "      <td>4.346734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.190100</td>\n",
       "      <td>0.104920</td>\n",
       "      <td>77.460500</td>\n",
       "      <td>58.746000</td>\n",
       "      <td>77.495100</td>\n",
       "      <td>77.496800</td>\n",
       "      <td>53.462300</td>\n",
       "      <td>4.346734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.174500</td>\n",
       "      <td>0.095059</td>\n",
       "      <td>74.617200</td>\n",
       "      <td>52.608600</td>\n",
       "      <td>74.650500</td>\n",
       "      <td>74.679400</td>\n",
       "      <td>51.039700</td>\n",
       "      <td>4.346734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.146200</td>\n",
       "      <td>0.088374</td>\n",
       "      <td>79.328600</td>\n",
       "      <td>63.469400</td>\n",
       "      <td>79.360000</td>\n",
       "      <td>79.307800</td>\n",
       "      <td>56.863400</td>\n",
       "      <td>4.346734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.118300</td>\n",
       "      <td>0.082198</td>\n",
       "      <td>82.974500</td>\n",
       "      <td>67.648400</td>\n",
       "      <td>82.954400</td>\n",
       "      <td>82.954300</td>\n",
       "      <td>61.300000</td>\n",
       "      <td>4.346734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.112700</td>\n",
       "      <td>0.113399</td>\n",
       "      <td>81.964900</td>\n",
       "      <td>66.721100</td>\n",
       "      <td>81.947300</td>\n",
       "      <td>81.977200</td>\n",
       "      <td>57.820200</td>\n",
       "      <td>4.346734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.145400</td>\n",
       "      <td>0.086789</td>\n",
       "      <td>82.249400</td>\n",
       "      <td>66.755100</td>\n",
       "      <td>82.217000</td>\n",
       "      <td>82.241700</td>\n",
       "      <td>59.097300</td>\n",
       "      <td>4.346734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.097800</td>\n",
       "      <td>0.076000</td>\n",
       "      <td>84.314100</td>\n",
       "      <td>71.298500</td>\n",
       "      <td>84.265400</td>\n",
       "      <td>84.348500</td>\n",
       "      <td>65.937500</td>\n",
       "      <td>4.346734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.075325</td>\n",
       "      <td>82.645600</td>\n",
       "      <td>71.208100</td>\n",
       "      <td>82.678900</td>\n",
       "      <td>82.694900</td>\n",
       "      <td>53.901200</td>\n",
       "      <td>4.346734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.088900</td>\n",
       "      <td>0.064349</td>\n",
       "      <td>86.099200</td>\n",
       "      <td>74.150100</td>\n",
       "      <td>86.052200</td>\n",
       "      <td>86.070000</td>\n",
       "      <td>68.344500</td>\n",
       "      <td>4.346734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.077800</td>\n",
       "      <td>0.062196</td>\n",
       "      <td>87.743400</td>\n",
       "      <td>77.009600</td>\n",
       "      <td>87.724100</td>\n",
       "      <td>87.702000</td>\n",
       "      <td>70.810500</td>\n",
       "      <td>4.346734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.076000</td>\n",
       "      <td>0.068779</td>\n",
       "      <td>85.277800</td>\n",
       "      <td>71.884700</td>\n",
       "      <td>85.215000</td>\n",
       "      <td>85.297800</td>\n",
       "      <td>65.255800</td>\n",
       "      <td>4.346734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.079100</td>\n",
       "      <td>0.059972</td>\n",
       "      <td>86.299700</td>\n",
       "      <td>74.350800</td>\n",
       "      <td>86.246700</td>\n",
       "      <td>86.289900</td>\n",
       "      <td>69.139600</td>\n",
       "      <td>4.346734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.068500</td>\n",
       "      <td>0.067800</td>\n",
       "      <td>86.120900</td>\n",
       "      <td>74.167400</td>\n",
       "      <td>86.084200</td>\n",
       "      <td>86.053700</td>\n",
       "      <td>68.197900</td>\n",
       "      <td>4.346734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.072300</td>\n",
       "      <td>0.052845</td>\n",
       "      <td>88.369700</td>\n",
       "      <td>77.870000</td>\n",
       "      <td>88.328700</td>\n",
       "      <td>88.356700</td>\n",
       "      <td>73.562800</td>\n",
       "      <td>4.346734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.060100</td>\n",
       "      <td>0.060629</td>\n",
       "      <td>86.568700</td>\n",
       "      <td>75.845900</td>\n",
       "      <td>86.645000</td>\n",
       "      <td>86.569800</td>\n",
       "      <td>69.663200</td>\n",
       "      <td>4.346734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.060400</td>\n",
       "      <td>0.053704</td>\n",
       "      <td>87.761300</td>\n",
       "      <td>77.599800</td>\n",
       "      <td>87.743800</td>\n",
       "      <td>87.717300</td>\n",
       "      <td>69.419200</td>\n",
       "      <td>4.346734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.055700</td>\n",
       "      <td>0.055473</td>\n",
       "      <td>87.372800</td>\n",
       "      <td>76.135900</td>\n",
       "      <td>87.352200</td>\n",
       "      <td>87.338100</td>\n",
       "      <td>70.640900</td>\n",
       "      <td>4.346734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.053300</td>\n",
       "      <td>0.049074</td>\n",
       "      <td>88.360700</td>\n",
       "      <td>77.874300</td>\n",
       "      <td>88.337900</td>\n",
       "      <td>88.331900</td>\n",
       "      <td>72.546600</td>\n",
       "      <td>4.346734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>0.057245</td>\n",
       "      <td>87.883500</td>\n",
       "      <td>76.529200</td>\n",
       "      <td>87.858600</td>\n",
       "      <td>87.846200</td>\n",
       "      <td>71.720700</td>\n",
       "      <td>4.346734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.059700</td>\n",
       "      <td>0.047671</td>\n",
       "      <td>88.746100</td>\n",
       "      <td>78.152800</td>\n",
       "      <td>88.759700</td>\n",
       "      <td>88.692100</td>\n",
       "      <td>74.415900</td>\n",
       "      <td>4.346734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>0.050693</td>\n",
       "      <td>88.863600</td>\n",
       "      <td>78.501100</td>\n",
       "      <td>88.823600</td>\n",
       "      <td>88.809500</td>\n",
       "      <td>73.517700</td>\n",
       "      <td>4.346734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.057800</td>\n",
       "      <td>0.045468</td>\n",
       "      <td>88.813200</td>\n",
       "      <td>78.708600</td>\n",
       "      <td>88.782300</td>\n",
       "      <td>88.751200</td>\n",
       "      <td>74.580500</td>\n",
       "      <td>4.346734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.055700</td>\n",
       "      <td>0.046580</td>\n",
       "      <td>88.884500</td>\n",
       "      <td>78.890400</td>\n",
       "      <td>88.909900</td>\n",
       "      <td>88.866500</td>\n",
       "      <td>75.315300</td>\n",
       "      <td>4.346734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.044800</td>\n",
       "      <td>0.047116</td>\n",
       "      <td>89.240600</td>\n",
       "      <td>79.564800</td>\n",
       "      <td>89.214400</td>\n",
       "      <td>89.155900</td>\n",
       "      <td>75.867900</td>\n",
       "      <td>4.346734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.053200</td>\n",
       "      <td>0.045738</td>\n",
       "      <td>89.474800</td>\n",
       "      <td>79.913100</td>\n",
       "      <td>89.469000</td>\n",
       "      <td>89.373700</td>\n",
       "      <td>76.254500</td>\n",
       "      <td>4.346734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangcheng/anaconda3/envs/swin/lib/python3.11/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "/home/wangcheng/anaconda3/envs/swin/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/wangcheng/anaconda3/envs/swin/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/wangcheng/anaconda3/envs/swin/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/wangcheng/anaconda3/envs/swin/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/wangcheng/anaconda3/envs/swin/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/wangcheng/anaconda3/envs/swin/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/wangcheng/anaconda3/envs/swin/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/wangcheng/anaconda3/envs/swin/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/wangcheng/anaconda3/envs/swin/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/wangcheng/anaconda3/envs/swin/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/wangcheng/anaconda3/envs/swin/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/wangcheng/anaconda3/envs/swin/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/wangcheng/anaconda3/envs/swin/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/wangcheng/anaconda3/envs/swin/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/wangcheng/anaconda3/envs/swin/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/wangcheng/anaconda3/envs/swin/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/wangcheng/anaconda3/envs/swin/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/wangcheng/anaconda3/envs/swin/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/wangcheng/anaconda3/envs/swin/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/wangcheng/anaconda3/envs/swin/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/wangcheng/anaconda3/envs/swin/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/wangcheng/anaconda3/envs/swin/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/wangcheng/anaconda3/envs/swin/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/wangcheng/anaconda3/envs/swin/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/wangcheng/anaconda3/envs/swin/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/wangcheng/anaconda3/envs/swin/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/wangcheng/anaconda3/envs/swin/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/wangcheng/anaconda3/envs/swin/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=564, training_loss=0.16676992431600043, metrics={'train_runtime': 2979.5882, 'train_samples_per_second': 3.027, 'train_steps_per_second': 0.189, 'total_flos': 1.637080974186578e+18, 'train_loss': 0.16676992431600043, 'epoch': 2.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
